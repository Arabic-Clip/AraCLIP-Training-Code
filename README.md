<br />
<p align="center">
  <h1 align="center">AraCLIP</h1>
  <h3 align="center">Cross-Lingual Learning for Effective Arabic Image Retrieval</h3>
  
  <p align="center">  
    <a href="https://huggingface.co/spaces/Arabic-Clip/AraCLIP">Live Demo</a>
    ·
    <a href="https://huggingface.co/Arabic-Clip">Pre-trained Models</a>
    ·
    <a href="https://github.com/Arabic-Clip/AraCLIP-Training-Code/issues">Report Bug</a>
  </p>
</p>



<!-- ABOUT THE PROJECT -->
## Overview
![Alt text](Images/methodology.jpg?raw=true "Title")


## Demo
A live demonstration of multilingual Text-Image retrieval using M-CLIP can be found [here!](https://huggingface.co/spaces/Arabic-Clip/AraCLIP) 



## Setting Up the Environment

To set up the environment, please use the `environment.yaml` file. This file contains all the necessary dependencies.



## Setting Up the Environment

To create the conda environment using the `environment.yaml` file, run the following command:

```bash
conda env create -f environment.yaml
```

## Pre-trained Models
Every text encoder is a [Arabic-Clip Huggingface Organization](https://huggingface.co/Arabic-Clip) available transformer, with an additional linear layer on top to be downloaded.


<!-- ACKNOWLEDGEMENTS -->
## Acknowledgements
* [Multilingual-CLIP](https://github.com/FreddeFrallan/Multilingual-CLIP)
* [Stability.ai](https://stability.ai/) for providing much appreciated compute during training.
* [CLIP](https://openai.com/blog/clip/)
* [OpenAI](https://openai.com/)
* [Huggingface](https://huggingface.co/)
* [Best Readme Template](https://github.com/othneildrew/Best-README-Template)
* ["Two Cats" Image by pl1602](https://search.creativecommons.org/photos/8dfd802b-58e5-4cc5-889d-96abba540de1)

<!-- LICENSE -->
## License
Distributed under the MIT License. See `LICENSE` for more information.


